# core/image_pdf_translator.py

import asyncio
from pathlib import Path
from loguru import logger
import shutil

# å„é˜¶æ®µæ¨¡å—ï¼ˆæ³¨æ„ï¼šä¸å†å¯¼å…¥ stage_splitterï¼‰
from .stages.mineru_processor import stage_mineru_processor
from .stages.leaf_extractor import stage_leaf_extractor
from .stages.translator import stage_translator
from .stages.blur_processor import stage_blur_processor
from .stages.html_renderer import stage_html_renderer
from .pipeline_message import PipelineMessage

# å·¥å…·å‡½æ•°ï¼šç”¨äºåˆå§‹åˆ†å‰²
from .pdf_preprocessor import preprocess_and_split_pdf

# æœ€ç»ˆåˆå¹¶å·¥å…·
from .pdf_final_merger import merge_all_final_pdfs


async def translate_image_pdf(
    pdf_path: str,
    output_dir: str,
    target_lang: str,
    api_key: str = None,
    model_name: str = None,
    base_url: str = None,
    final_output_dir: str = None,
    max_concurrent_translate: int = 10,
    mineru_api_key=None,
    mineru_base_url=None,
    pdf_type: str = "txt",
    chunk_size: int = 25,
    max_concurrent_mineru: int = 1,
    cleanup_workdir: bool = False,
    max_retry: int = 3,
    **kwargs
):
    """
    ä¸»å…¥å£ï¼šå¯åŠ¨å¯é‡è¯•çš„æµæ°´çº¿ã€‚
    åˆ†å‰²åªæ‰§è¡Œä¸€æ¬¡ï¼Œåç»­å¤±è´¥æ—¶ä» mineru é˜¶æ®µé‡è¯•ã€‚
    """
    pdf_path = Path(pdf_path).resolve()
    if not pdf_path.exists():
        return {"success": False, "error": f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"}

    project_root = Path(__file__).parent.parent.resolve()
    workdir = (project_root / "workdir" / pdf_path.stem).resolve()
    workdir.mkdir(parents=True, exist_ok=True)

    final_output_path_obj = Path(final_output_dir or output_dir).resolve()
    final_output_path_obj.mkdir(parents=True, exist_ok=True)

    logger.info(f"ğŸ“„ å¼€å§‹å¤„ç† PDF: {pdf_path.name}")
    logger.info(f"âš™ï¸ ä½¿ç”¨æ¨¡å¼: {pdf_type}")

    # === Step 1: æ‰§è¡Œä¸€æ¬¡åˆ†å‰²ï¼ˆä¸æ”¾å…¥æµæ°´çº¿ï¼‰===
    loop = asyncio.get_running_loop()
    try:
        chunk_paths = await loop.run_in_executor(
            None,
            preprocess_and_split_pdf,
            pdf_path,
            workdir,
            chunk_size
        )
    except Exception as e:
        logger.error(f"âŒ åˆ†å‰²é˜¶æ®µå¼‚å¸¸: {e}")
        return {"success": False, "error": f"åˆ†å‰²å¤±è´¥: {e}"}

    total_chunks = len(chunk_paths)
    if total_chunks == 0:
        return {"success": False, "error": "PDF åˆ†å‰²åæ— æœ‰æ•ˆ chunk"}

    logger.info(f"âœ‚ï¸ PDF å·²åˆ†å‰²ä¸º {total_chunks} ä¸ª chunkï¼Œå‡†å¤‡è¿›å…¥å¤„ç†æµæ°´çº¿")

    # === Step 2: é‡è¯•å¾ªç¯ ===
    for attempt in range(1, max_retry + 1):
        logger.info(f"ğŸ” ç¬¬ {attempt}/{max_retry} æ¬¡å°è¯•å¤„ç† {total_chunks} ä¸ª chunk")

        # === é˜Ÿåˆ—å®šä¹‰ï¼ˆä» mineru å¼€å§‹ï¼‰===
        q_mineru_to_leaf = asyncio.Queue()
        q_leaf_to_translate = asyncio.Queue()
        q_translate_to_blur = asyncio.Queue()
        q_blur_to_html = asyncio.Queue()
        q_html_to_pdf = asyncio.Queue()

        # === æ„é€  mineru çš„è¾“å…¥é˜Ÿåˆ—ï¼ˆå…³é”®ï¼ï¼‰===
        q_splitter_to_mineru = asyncio.Queue()
        for chunk_path in chunk_paths:
            msg = PipelineMessage(chunk_path)
            msg.pdf_type = pdf_type
            msg.total_chunks = total_chunks  # â† å…±äº«æ€»æ•°
            await q_splitter_to_mineru.put(msg)
        await q_splitter_to_mineru.put(None)  # ç»“æŸä¿¡å·

        # === å¯åŠ¨ä» mineru åˆ° html_renderer çš„ä»»åŠ¡ ===
        tasks = [
            asyncio.create_task(stage_mineru_processor(
                q_splitter_to_mineru, q_mineru_to_leaf,
                workdir / "mineru_results", pdf_type,
                max_concurrent_mineru, mineru_api_key, mineru_base_url
            )),
            asyncio.create_task(stage_leaf_extractor(q_mineru_to_leaf, q_leaf_to_translate, pdf_type)),
            asyncio.create_task(stage_translator(
                q_leaf_to_translate, q_translate_to_blur,
                target_lang, api_key, base_url, model_name, max_concurrent_translate
            )),
            asyncio.create_task(stage_blur_processor(q_translate_to_blur, q_blur_to_html)),
            asyncio.create_task(stage_html_renderer(q_blur_to_html, q_html_to_pdf, kwargs)),
        ]

        try:
            await asyncio.gather(*tasks, return_exceptions=True)
        except Exception as e:
            logger.warning(f"ç¬¬ {attempt} æ¬¡æµæ°´çº¿æ‰§è¡Œå¼‚å¸¸ï¼ˆç»§ç»­é‡è¯•ï¼‰: {e}")

        # === æ”¶é›†æœ€ç»ˆç”Ÿæˆçš„ PDF è·¯å¾„ ===
        final_pdfs = []
        received_stems = set()

        while not q_html_to_pdf.empty():
            try:
                msg = q_html_to_pdf.get_nowait()
                if msg is None:
                    continue
                if msg.pdf_path and Path(msg.pdf_path).is_file():
                    final_pdfs.append(str(msg.pdf_path))
                    received_stems.add(msg.chunk_stem)
            except asyncio.QueueEmpty:
                break


        actual_count = len(final_pdfs)
        logger.info(f"ğŸ“Š æœ¬æ¬¡å°è¯•ç”Ÿæˆäº† {actual_count} / {total_chunks} ä¸ªæœ€ç»ˆ PDF")

        if actual_count == total_chunks:
            # âœ… æ•°é‡ä¸€è‡´ï¼Œæ‰§è¡Œæœ€ç»ˆåˆå¹¶
            merge_result = merge_all_final_pdfs(
                file_list=final_pdfs,
                output_path=str(final_output_path_obj / f"{pdf_path.stem}_translated.pdf")
            )

            if merge_result["success"]:
                final_pdf_path = Path(merge_result["output_path"])

                if cleanup_workdir:
                    try:
                        if workdir.exists():
                            shutil.rmtree(workdir)
                            logger.info(f"ğŸ§¹ å·¥ä½œåŒºå·²æ¸…é™¤: {workdir}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ¸…ç†å·¥ä½œåŒºå¤±è´¥ï¼ˆä½†æµç¨‹å·²æˆåŠŸï¼‰: {e}")
                else:
                    logger.info(f"ğŸ” è°ƒè¯•æ¨¡å¼ï¼šä¿ç•™å·¥ä½œåŒº {workdir}")

                return {
                    "success": True,
                    "output_path": str(final_pdf_path),
                    "merged_pdf_path": str(final_pdf_path),
                    "message": "Pipeline completed successfully."
                }
            else:
                logger.error(f"âŒ åˆå¹¶å¤±è´¥: {merge_result['error']}")
        else:
            missing_count = total_chunks - actual_count
            logger.warning(f"ğŸŸ¡ ç¼ºå¤± {missing_count} ä¸ª chunk çš„æœ€ç»ˆ PDFï¼Œå‡†å¤‡é‡è¯•...")

        # é‡è¯•å‰ç­‰å¾…ï¼ˆå¯é€‰ï¼‰
        if attempt < max_retry:
            await asyncio.sleep(1)

    # === æ‰€æœ‰é‡è¯•å¤±è´¥ ===
    return {
        "success": False,
        "error": f"ç»è¿‡ {max_retry} æ¬¡é‡è¯•ï¼Œä»æœªèƒ½ç”Ÿæˆå®Œæ•´çš„ {total_chunks} ä¸ª PDFï¼ˆæœ€åä¸€æ¬¡ä»…ç”Ÿæˆ {actual_count} ä¸ªï¼‰"
    }